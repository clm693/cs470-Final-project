{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file (panda, numpy)\n",
        "2. split dataset. Example code:()\n",
        "   ```\n",
        "   random.shuffle(data) # change if you are using pandas dataframe\n",
        "   training = data[:int(len(data)*0.8)]\n",
        "   test = data[int(len(data)*0.8):]\n",
        "\n",
        "   fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "   for train_idx, val_idx in fold5.split(training):\n",
        "      sub_val = training[val_idx]\n",
        "      sub_train = training[train_idx]\n",
        "      clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "      performance(clf, test) # test the model on test dataset\n",
        "   ```"
      ],
      "metadata": {
        "id": "uxgBX0YXu1du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "data = pd.read_csv(\"spambase.csv\")\n",
        "\n",
        "data.sample(frac=1) # change if you are using pandas dataframe\n",
        "training = data[:int(len(data)*0.8)]\n",
        "test = data[int(len(data)*0.8):]\n",
        "\n",
        "fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "\n",
        "for train_idx, val_idx in fold5.split(training):\n",
        "   sub_val = training[val_idx]\n",
        "   sub_train = training[train_idx]\n",
        "   clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "   performance(clf, test) # test the model on test dataset"
      ],
      "metadata": {
        "id": "RH1dibW2NeE4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive bayes\n",
        "\n",
        "1. model learning:\n",
        "\n",
        "   Note:\n",
        "\n",
        "   features: remove attributes that is not related to word (the last four attributes)\n",
        "\n",
        "   labels: the last column\n",
        "\n",
        "   count P(c) -> how many samples are positive, and how many are negtive\n",
        "\n",
        "   if freq_word>0, then this word exists. You could use this to calculate P(a|c) -> for each class, what is the prob of each word\n",
        "\n",
        "   remember to use laplace smoothing.\n",
        "\n",
        "2. model evaluation (on val dataset -> performance(model, val)):\n",
        "   \n",
        "   for each new sample, $\\prod{P(a|c)}P(c)$ if word is in the email(freq_word > 0); and find the maximum class\n",
        "   \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "FXyRfd35yRPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "\n",
        "def separate_by_class(data):\n",
        "   sep_list = dict()\n",
        "   for i in range(len(data)-4):\n",
        "      arr = data.iloc[i]\n",
        "      class_val = arr[-1]\n",
        "   if (class_val not in sep_list):\n",
        "      sep_list[class_val] = list()\n",
        "      sep_list[class_val].append(arr)\n",
        "   return sep_list\n",
        "\n",
        "def stdev(data):\n",
        "    avg = sum(data)/len(data.index)\n",
        "    z = 0\n",
        "    for i in data:\n",
        "      z += (i-avg) ** 2\n",
        "    z /= float(data.shape[0]-1)\n",
        "    return z ** 0.5\n",
        "\n",
        "def summarize_dataset(data):\n",
        "   sum_list = []\n",
        "   for col in zip(*data):\n",
        "      sum_list.append(sum(data)/len(data.index), stdev(col), len(col.index))\n",
        "   return sum_list\n",
        "\n",
        "def summarize_by_class(dataset):\n",
        "   sep_list = separate_by_class(dataset)\n",
        "   sum_list = dict()\n",
        "   for class_val, rows in sep_list.items():\n",
        "      sum_list[class_val] = summarize_dataset(rows)\n",
        "   return sum_list\n",
        "\n",
        "def calculate_probability(x, avg, stdev):\n",
        "   exponent = exp(-((x-avg)**2 / (2 * stdev ** 2 )))\n",
        "   return exponent * (1 / ((2 * 3.1415926535) ** 0.5) * stdev)\n",
        "\n",
        "def calculate_class_probabilities(row, sum_list):\n",
        "   for i in sum_list:\n",
        "      rows += sum_list[i][0][2]\n",
        "   prob_list = dict()\n",
        "   for class_val, sum_list in sum_list.items():\n",
        "     prob_list[class_val] = sum_list[class_val][0][2]/rows\n",
        "   for i in range(len(sum_list)):\n",
        "     avg, stdev, count = sum_list[i]\n",
        "     prob_list[class_val] *= calculate_probability(row[i], avg, stdev)\n",
        "   return prob_list\n",
        "\n",
        "def predict(row, sum_list):\n",
        "   prob_list = calculate_class_probabilities(sum_list, row)\n",
        "   best_label, best_prob = None, -1\n",
        "   for class_value, probability in prob_list.items():\n",
        "      if best_label is None or probability > best_prob:\n",
        "         best_prob = probability\n",
        "   best_label = class_value\n",
        "   return best_label\n",
        "\n",
        "def naive_model(training_data, test_data):\n",
        "   sum_list = summarize_by_class(training_data)\n",
        "   predictions = list()\n",
        "   for row in test:\n",
        "      output = predict(row, sum_list)\n",
        "      predictions.append(output)\n",
        "   return(predictions)\n",
        "\n",
        "print(naive_model(training, test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "da-uVfRECSt5",
        "outputId": "70479726-d570-42b6-db11-ccf709c82c1f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'builtin_function_or_method' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9ec4b4019658>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-9ec4b4019658>\u001b[0m in \u001b[0;36mnaive_model\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnaive_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m    \u001b[0msum_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_by_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m    \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-9ec4b4019658>\u001b[0m in \u001b[0;36msummarize_by_class\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     29\u001b[0m    \u001b[0msum_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mclass_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msep_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0msum_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msum_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-9ec4b4019658>\u001b[0m in \u001b[0;36msummarize_dataset\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     22\u001b[0m    \u001b[0msum_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0msum_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msum_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'builtin_function_or_method' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN\n",
        "1. model learning: None\n",
        "\n",
        "2. model evaluation(on val dataset): You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "\n",
        "   ```\n",
        "   Note:\n",
        "   parallel programing\n",
        "   numpy.cos() to calcuate the similarity\n",
        "   ```"
      ],
      "metadata": {
        "id": "5jRvHTlW0DYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euc_dist(row1, row2):\n",
        "   dist = 0\n",
        "   for i in range(len(row1)-1):\n",
        "      dist += (row1[i] - row2[i]) ** 2\n",
        "   return dist ** 0.5\n",
        "\n",
        "def get_neighbors(training_data, test_row, num_neighbors):\n",
        "   dist_list = list()\n",
        "   for training_row in training_data:\n",
        "      dist = euc_dist(test_row, training_row)\n",
        "      dist_list.append((training_row, dist))\n",
        "      dist_list.sort(key=lambda tup: tup[1])\n",
        "   neighbor_list = list()\n",
        "   for i in range(num_neighbors):\n",
        "      neighbor_list.append(dist_list[i][0])\n",
        "   return neighbor_list\n",
        "\n",
        "def predict_classification(training_data, test_row, num_neighbors):\n",
        "   neighbors = get_neighbors(training_data, test_row, num_neighbors)\n",
        "   outputs = []\n",
        "   for i in neighbors:\n",
        "      outputs.append(i[-1])\n",
        "   prediction = max(set(outputs), key=outputs.count)\n",
        "   return prediction\n",
        "\n",
        "def k_nearest_neighbors(training_data, test_data, num_neighbors):\n",
        "   prob_list = list()\n",
        "   for row in test_data:\n",
        "      output = predict_classification(training_data, row, num_neighbors)\n",
        "      prob_list.append(output)\n",
        "   return(prob_list)"
      ],
      "metadata": {
        "id": "pPbge6pYgaFx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR\n",
        "\n",
        "1. model learning: You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "    \n",
        "    $y = sigmoid(MX)$\n",
        "\n",
        "step 1: add one more column (all value is 1) in X -> X' = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "step 2:vector M = np.random.randn(len(X[0])+1, 1);\n",
        "\n",
        "key formula for step 3 (Note: n is the size of the TRAINING dataset; $cdot$ is dot production ):\n",
        "\n",
        "1. $pred_y = sigmoid(M\\cdot X')$\n",
        "\n",
        "2. $loss = -\\sum(y\\cdot log(pred_y)+(1-y)\\cdot log(1-pred_y))/n$\n",
        "\n",
        "3. $gm=X'\\cdot (pred_y - y)*2/n$\n",
        "\n",
        "Step 3 example code:\n",
        "   ```\n",
        "   #Step 3: performing gradient descent on whole dataset:\n",
        "   best_model = M\n",
        "   best_performace = 0\n",
        "   for i in range(epoch):\n",
        "     pred_y = ...\n",
        "     gm = ...\n",
        "     _p = performace(model, val)\n",
        "     if _p > best_performance:\n",
        "        best_model = M\n",
        "        best_performance = _p\n",
        "     M = M - learning_rate*gm\n",
        "   ```\n",
        "\n",
        "2. model evaluation(on val dataset):\n",
        "  \n",
        "   calculate pred_y, if more than 0.5, then the predicted label is 1."
      ],
      "metadata": {
        "id": "OUzUupva0Fxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(row, coefficients):\n",
        "   predY = coefficients[0]\n",
        "   for i in range(len(row)-1):\n",
        "      predY += coefficients[i + 1] * row[i]\n",
        "   return predY\n",
        "\n",
        "def coefficients_sgd(training_data, l_rate, n_epoch):\n",
        "   coeff = []\n",
        "   for i in range(len(training_data[0])):\n",
        "      coeff.append(0)\n",
        "   for epoch in range(n_epoch):\n",
        "      sum_error = 0\n",
        "      for row in training_data:\n",
        "         yhat = predict(row, coeff)\n",
        "      error = yhat - row[-1]\n",
        "      sum_error += error**2\n",
        "   coeff[0] = coeff[0] - l_rate * error\n",
        "   for i in range(len(row)-1):\n",
        "      coeff[i + 1] = coeff[i + 1] - l_rate * error * row[i]\n",
        "   return coeff\n",
        "\n",
        "def linear_regression_sgd(training_data, test_data, l_rate, n_epoch):\n",
        "   predictions = list()\n",
        "   coeff = coefficients_sgd(training_data, l_rate, n_epoch)\n",
        "   for row in test_data:\n",
        "      predY = predict(row, coeff)\n",
        "      predictions.append(predY)\n",
        "   return(predictions)"
      ],
      "metadata": {
        "id": "uuijK8bYiiSG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ],
      "metadata": {
        "id": "mAssSW_I0GvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def performance(model, data):\n",
        "  pred_list = []\n",
        "  other_list = []\n",
        "  result = 0\n",
        "  for i in range(len(data.index)):\n",
        "     pred_list.append(predict(i))\n",
        "\n",
        "  for i in range(len(pred_list)):\n",
        "     if(pred_list[i] == other_list[i]):\n",
        "        result = result + 1\n",
        "  return result/len(pred_list)"
      ],
      "metadata": {
        "id": "e0MQ0eo0MnmB"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}